// Code generated by SQLBoiler 4.19.7 (https://github.com/aarondl/sqlboiler). DO NOT EDIT.
// This file is meant to be re-generated in place and/or deleted at any time.

package sqlboiler

import (
	"context"
	"database/sql"
	"fmt"
	"reflect"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/aarondl/null/v8"
	"github.com/aarondl/sqlboiler/v4/boil"
	"github.com/aarondl/sqlboiler/v4/queries"
	"github.com/aarondl/sqlboiler/v4/queries/qm"
	"github.com/aarondl/sqlboiler/v4/queries/qmhelper"
	"github.com/aarondl/strmangle"
	"github.com/friendsofgo/errors"
)

// IndexedDocument is an object representing the database table.
type IndexedDocument struct {
	ID string `boil:"id" json:"id" toml:"id" yaml:"id"`
	// Foreign key to analytics.post_analytics.id - unique identifier for the analytics post
	AnalyticsID string `boil:"analytics_id" json:"analytics_id" toml:"analytics_id" yaml:"analytics_id"`
	ProjectID   string `boil:"project_id" json:"project_id" toml:"project_id" yaml:"project_id"`
	SourceID    string `boil:"source_id" json:"source_id" toml:"source_id" yaml:"source_id"`
	// The vector point ID in Qdrant (usually matches analytics_id)
	QdrantPointID  string `boil:"qdrant_point_id" json:"qdrant_point_id" toml:"qdrant_point_id" yaml:"qdrant_point_id"`
	CollectionName string `boil:"collection_name" json:"collection_name" toml:"collection_name" yaml:"collection_name"`
	// SHA-256 hash of the document content for duplicate detection across different sources
	ContentHash string `boil:"content_hash" json:"content_hash" toml:"content_hash" yaml:"content_hash"`
	// PENDING: Awaiting processing, INDEXED: Successfully indexed, FAILED: Error during indexing, RE_INDEXING: Currently being re-indexed
	Status       string      `boil:"status" json:"status" toml:"status" yaml:"status"`
	ErrorMessage null.String `boil:"error_message" json:"error_message,omitempty" toml:"error_message" yaml:"error_message,omitempty"`
	RetryCount   null.Int    `boil:"retry_count" json:"retry_count,omitempty" toml:"retry_count" yaml:"retry_count,omitempty"`
	BatchID      null.String `boil:"batch_id" json:"batch_id,omitempty" toml:"batch_id" yaml:"batch_id,omitempty"`
	// How the data was ingested: kafka (from Kafka topic) or api (from HTTP endpoint)
	IngestionMethod string    `boil:"ingestion_method" json:"ingestion_method" toml:"ingestion_method" yaml:"ingestion_method"`
	EmbeddingTimeMS null.Int  `boil:"embedding_time_ms" json:"embedding_time_ms,omitempty" toml:"embedding_time_ms" yaml:"embedding_time_ms,omitempty"`
	UpsertTimeMS    null.Int  `boil:"upsert_time_ms" json:"upsert_time_ms,omitempty" toml:"upsert_time_ms" yaml:"upsert_time_ms,omitempty"`
	TotalTimeMS     null.Int  `boil:"total_time_ms" json:"total_time_ms,omitempty" toml:"total_time_ms" yaml:"total_time_ms,omitempty"`
	IndexedAt       null.Time `boil:"indexed_at" json:"indexed_at,omitempty" toml:"indexed_at" yaml:"indexed_at,omitempty"`
	CreatedAt       null.Time `boil:"created_at" json:"created_at,omitempty" toml:"created_at" yaml:"created_at,omitempty"`
	UpdatedAt       null.Time `boil:"updated_at" json:"updated_at,omitempty" toml:"updated_at" yaml:"updated_at,omitempty"`

	R *indexedDocumentR `boil:"-" json:"-" toml:"-" yaml:"-"`
	L indexedDocumentL  `boil:"-" json:"-" toml:"-" yaml:"-"`
}

var IndexedDocumentColumns = struct {
	ID              string
	AnalyticsID     string
	ProjectID       string
	SourceID        string
	QdrantPointID   string
	CollectionName  string
	ContentHash     string
	Status          string
	ErrorMessage    string
	RetryCount      string
	BatchID         string
	IngestionMethod string
	EmbeddingTimeMS string
	UpsertTimeMS    string
	TotalTimeMS     string
	IndexedAt       string
	CreatedAt       string
	UpdatedAt       string
}{
	ID:              "id",
	AnalyticsID:     "analytics_id",
	ProjectID:       "project_id",
	SourceID:        "source_id",
	QdrantPointID:   "qdrant_point_id",
	CollectionName:  "collection_name",
	ContentHash:     "content_hash",
	Status:          "status",
	ErrorMessage:    "error_message",
	RetryCount:      "retry_count",
	BatchID:         "batch_id",
	IngestionMethod: "ingestion_method",
	EmbeddingTimeMS: "embedding_time_ms",
	UpsertTimeMS:    "upsert_time_ms",
	TotalTimeMS:     "total_time_ms",
	IndexedAt:       "indexed_at",
	CreatedAt:       "created_at",
	UpdatedAt:       "updated_at",
}

var IndexedDocumentTableColumns = struct {
	ID              string
	AnalyticsID     string
	ProjectID       string
	SourceID        string
	QdrantPointID   string
	CollectionName  string
	ContentHash     string
	Status          string
	ErrorMessage    string
	RetryCount      string
	BatchID         string
	IngestionMethod string
	EmbeddingTimeMS string
	UpsertTimeMS    string
	TotalTimeMS     string
	IndexedAt       string
	CreatedAt       string
	UpdatedAt       string
}{
	ID:              "indexed_documents.id",
	AnalyticsID:     "indexed_documents.analytics_id",
	ProjectID:       "indexed_documents.project_id",
	SourceID:        "indexed_documents.source_id",
	QdrantPointID:   "indexed_documents.qdrant_point_id",
	CollectionName:  "indexed_documents.collection_name",
	ContentHash:     "indexed_documents.content_hash",
	Status:          "indexed_documents.status",
	ErrorMessage:    "indexed_documents.error_message",
	RetryCount:      "indexed_documents.retry_count",
	BatchID:         "indexed_documents.batch_id",
	IngestionMethod: "indexed_documents.ingestion_method",
	EmbeddingTimeMS: "indexed_documents.embedding_time_ms",
	UpsertTimeMS:    "indexed_documents.upsert_time_ms",
	TotalTimeMS:     "indexed_documents.total_time_ms",
	IndexedAt:       "indexed_documents.indexed_at",
	CreatedAt:       "indexed_documents.created_at",
	UpdatedAt:       "indexed_documents.updated_at",
}

// Generated where

type whereHelperstring struct{ field string }

func (w whereHelperstring) EQ(x string) qm.QueryMod      { return qmhelper.Where(w.field, qmhelper.EQ, x) }
func (w whereHelperstring) NEQ(x string) qm.QueryMod     { return qmhelper.Where(w.field, qmhelper.NEQ, x) }
func (w whereHelperstring) LT(x string) qm.QueryMod      { return qmhelper.Where(w.field, qmhelper.LT, x) }
func (w whereHelperstring) LTE(x string) qm.QueryMod     { return qmhelper.Where(w.field, qmhelper.LTE, x) }
func (w whereHelperstring) GT(x string) qm.QueryMod      { return qmhelper.Where(w.field, qmhelper.GT, x) }
func (w whereHelperstring) GTE(x string) qm.QueryMod     { return qmhelper.Where(w.field, qmhelper.GTE, x) }
func (w whereHelperstring) LIKE(x string) qm.QueryMod    { return qm.Where(w.field+" LIKE ?", x) }
func (w whereHelperstring) NLIKE(x string) qm.QueryMod   { return qm.Where(w.field+" NOT LIKE ?", x) }
func (w whereHelperstring) ILIKE(x string) qm.QueryMod   { return qm.Where(w.field+" ILIKE ?", x) }
func (w whereHelperstring) NILIKE(x string) qm.QueryMod  { return qm.Where(w.field+" NOT ILIKE ?", x) }
func (w whereHelperstring) SIMILAR(x string) qm.QueryMod { return qm.Where(w.field+" SIMILAR TO ?", x) }
func (w whereHelperstring) NSIMILAR(x string) qm.QueryMod {
	return qm.Where(w.field+" NOT SIMILAR TO ?", x)
}
func (w whereHelperstring) IN(slice []string) qm.QueryMod {
	values := make([]any, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereIn(fmt.Sprintf("%s IN ?", w.field), values...)
}
func (w whereHelperstring) NIN(slice []string) qm.QueryMod {
	values := make([]any, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereNotIn(fmt.Sprintf("%s NOT IN ?", w.field), values...)
}

type whereHelpernull_String struct{ field string }

func (w whereHelpernull_String) EQ(x null.String) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, false, x)
}
func (w whereHelpernull_String) NEQ(x null.String) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, true, x)
}
func (w whereHelpernull_String) LT(x null.String) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LT, x)
}
func (w whereHelpernull_String) LTE(x null.String) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LTE, x)
}
func (w whereHelpernull_String) GT(x null.String) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GT, x)
}
func (w whereHelpernull_String) GTE(x null.String) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GTE, x)
}
func (w whereHelpernull_String) LIKE(x null.String) qm.QueryMod {
	return qm.Where(w.field+" LIKE ?", x)
}
func (w whereHelpernull_String) NLIKE(x null.String) qm.QueryMod {
	return qm.Where(w.field+" NOT LIKE ?", x)
}
func (w whereHelpernull_String) ILIKE(x null.String) qm.QueryMod {
	return qm.Where(w.field+" ILIKE ?", x)
}
func (w whereHelpernull_String) NILIKE(x null.String) qm.QueryMod {
	return qm.Where(w.field+" NOT ILIKE ?", x)
}
func (w whereHelpernull_String) SIMILAR(x null.String) qm.QueryMod {
	return qm.Where(w.field+" SIMILAR TO ?", x)
}
func (w whereHelpernull_String) NSIMILAR(x null.String) qm.QueryMod {
	return qm.Where(w.field+" NOT SIMILAR TO ?", x)
}
func (w whereHelpernull_String) IN(slice []string) qm.QueryMod {
	values := make([]any, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereIn(fmt.Sprintf("%s IN ?", w.field), values...)
}
func (w whereHelpernull_String) NIN(slice []string) qm.QueryMod {
	values := make([]any, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereNotIn(fmt.Sprintf("%s NOT IN ?", w.field), values...)
}

func (w whereHelpernull_String) IsNull() qm.QueryMod    { return qmhelper.WhereIsNull(w.field) }
func (w whereHelpernull_String) IsNotNull() qm.QueryMod { return qmhelper.WhereIsNotNull(w.field) }

type whereHelpernull_Int struct{ field string }

func (w whereHelpernull_Int) EQ(x null.Int) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, false, x)
}
func (w whereHelpernull_Int) NEQ(x null.Int) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, true, x)
}
func (w whereHelpernull_Int) LT(x null.Int) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LT, x)
}
func (w whereHelpernull_Int) LTE(x null.Int) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LTE, x)
}
func (w whereHelpernull_Int) GT(x null.Int) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GT, x)
}
func (w whereHelpernull_Int) GTE(x null.Int) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GTE, x)
}
func (w whereHelpernull_Int) IN(slice []int) qm.QueryMod {
	values := make([]any, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereIn(fmt.Sprintf("%s IN ?", w.field), values...)
}
func (w whereHelpernull_Int) NIN(slice []int) qm.QueryMod {
	values := make([]any, 0, len(slice))
	for _, value := range slice {
		values = append(values, value)
	}
	return qm.WhereNotIn(fmt.Sprintf("%s NOT IN ?", w.field), values...)
}

func (w whereHelpernull_Int) IsNull() qm.QueryMod    { return qmhelper.WhereIsNull(w.field) }
func (w whereHelpernull_Int) IsNotNull() qm.QueryMod { return qmhelper.WhereIsNotNull(w.field) }

type whereHelpernull_Time struct{ field string }

func (w whereHelpernull_Time) EQ(x null.Time) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, false, x)
}
func (w whereHelpernull_Time) NEQ(x null.Time) qm.QueryMod {
	return qmhelper.WhereNullEQ(w.field, true, x)
}
func (w whereHelpernull_Time) LT(x null.Time) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LT, x)
}
func (w whereHelpernull_Time) LTE(x null.Time) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.LTE, x)
}
func (w whereHelpernull_Time) GT(x null.Time) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GT, x)
}
func (w whereHelpernull_Time) GTE(x null.Time) qm.QueryMod {
	return qmhelper.Where(w.field, qmhelper.GTE, x)
}

func (w whereHelpernull_Time) IsNull() qm.QueryMod    { return qmhelper.WhereIsNull(w.field) }
func (w whereHelpernull_Time) IsNotNull() qm.QueryMod { return qmhelper.WhereIsNotNull(w.field) }

var IndexedDocumentWhere = struct {
	ID              whereHelperstring
	AnalyticsID     whereHelperstring
	ProjectID       whereHelperstring
	SourceID        whereHelperstring
	QdrantPointID   whereHelperstring
	CollectionName  whereHelperstring
	ContentHash     whereHelperstring
	Status          whereHelperstring
	ErrorMessage    whereHelpernull_String
	RetryCount      whereHelpernull_Int
	BatchID         whereHelpernull_String
	IngestionMethod whereHelperstring
	EmbeddingTimeMS whereHelpernull_Int
	UpsertTimeMS    whereHelpernull_Int
	TotalTimeMS     whereHelpernull_Int
	IndexedAt       whereHelpernull_Time
	CreatedAt       whereHelpernull_Time
	UpdatedAt       whereHelpernull_Time
}{
	ID:              whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"id\""},
	AnalyticsID:     whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"analytics_id\""},
	ProjectID:       whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"project_id\""},
	SourceID:        whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"source_id\""},
	QdrantPointID:   whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"qdrant_point_id\""},
	CollectionName:  whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"collection_name\""},
	ContentHash:     whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"content_hash\""},
	Status:          whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"status\""},
	ErrorMessage:    whereHelpernull_String{field: "\"schema_knowledge\".\"indexed_documents\".\"error_message\""},
	RetryCount:      whereHelpernull_Int{field: "\"schema_knowledge\".\"indexed_documents\".\"retry_count\""},
	BatchID:         whereHelpernull_String{field: "\"schema_knowledge\".\"indexed_documents\".\"batch_id\""},
	IngestionMethod: whereHelperstring{field: "\"schema_knowledge\".\"indexed_documents\".\"ingestion_method\""},
	EmbeddingTimeMS: whereHelpernull_Int{field: "\"schema_knowledge\".\"indexed_documents\".\"embedding_time_ms\""},
	UpsertTimeMS:    whereHelpernull_Int{field: "\"schema_knowledge\".\"indexed_documents\".\"upsert_time_ms\""},
	TotalTimeMS:     whereHelpernull_Int{field: "\"schema_knowledge\".\"indexed_documents\".\"total_time_ms\""},
	IndexedAt:       whereHelpernull_Time{field: "\"schema_knowledge\".\"indexed_documents\".\"indexed_at\""},
	CreatedAt:       whereHelpernull_Time{field: "\"schema_knowledge\".\"indexed_documents\".\"created_at\""},
	UpdatedAt:       whereHelpernull_Time{field: "\"schema_knowledge\".\"indexed_documents\".\"updated_at\""},
}

// IndexedDocumentRels is where relationship names are stored.
var IndexedDocumentRels = struct {
}{}

// indexedDocumentR is where relationships are stored.
type indexedDocumentR struct {
}

// NewStruct creates a new relationship struct
func (*indexedDocumentR) NewStruct() *indexedDocumentR {
	return &indexedDocumentR{}
}

// indexedDocumentL is where Load methods for each relationship are stored.
type indexedDocumentL struct{}

var (
	indexedDocumentAllColumns            = []string{"id", "analytics_id", "project_id", "source_id", "qdrant_point_id", "collection_name", "content_hash", "status", "error_message", "retry_count", "batch_id", "ingestion_method", "embedding_time_ms", "upsert_time_ms", "total_time_ms", "indexed_at", "created_at", "updated_at"}
	indexedDocumentColumnsWithoutDefault = []string{"analytics_id", "project_id", "source_id", "qdrant_point_id", "collection_name", "content_hash", "ingestion_method"}
	indexedDocumentColumnsWithDefault    = []string{"id", "status", "error_message", "retry_count", "batch_id", "embedding_time_ms", "upsert_time_ms", "total_time_ms", "indexed_at", "created_at", "updated_at"}
	indexedDocumentPrimaryKeyColumns     = []string{"id"}
	indexedDocumentGeneratedColumns      = []string{}
)

type (
	// IndexedDocumentSlice is an alias for a slice of pointers to IndexedDocument.
	// This should almost always be used instead of []IndexedDocument.
	IndexedDocumentSlice []*IndexedDocument
	// IndexedDocumentHook is the signature for custom IndexedDocument hook methods
	IndexedDocumentHook func(context.Context, boil.ContextExecutor, *IndexedDocument) error

	indexedDocumentQuery struct {
		*queries.Query
	}
)

// Cache for insert, update and upsert
var (
	indexedDocumentType                 = reflect.TypeOf(&IndexedDocument{})
	indexedDocumentMapping              = queries.MakeStructMapping(indexedDocumentType)
	indexedDocumentPrimaryKeyMapping, _ = queries.BindMapping(indexedDocumentType, indexedDocumentMapping, indexedDocumentPrimaryKeyColumns)
	indexedDocumentInsertCacheMut       sync.RWMutex
	indexedDocumentInsertCache          = make(map[string]insertCache)
	indexedDocumentUpdateCacheMut       sync.RWMutex
	indexedDocumentUpdateCache          = make(map[string]updateCache)
	indexedDocumentUpsertCacheMut       sync.RWMutex
	indexedDocumentUpsertCache          = make(map[string]insertCache)
)

var (
	// Force time package dependency for automated UpdatedAt/CreatedAt.
	_ = time.Second
	// Force qmhelper dependency for where clause generation (which doesn't
	// always happen)
	_ = qmhelper.Where
)

var indexedDocumentAfterSelectMu sync.Mutex
var indexedDocumentAfterSelectHooks []IndexedDocumentHook

var indexedDocumentBeforeInsertMu sync.Mutex
var indexedDocumentBeforeInsertHooks []IndexedDocumentHook
var indexedDocumentAfterInsertMu sync.Mutex
var indexedDocumentAfterInsertHooks []IndexedDocumentHook

var indexedDocumentBeforeUpdateMu sync.Mutex
var indexedDocumentBeforeUpdateHooks []IndexedDocumentHook
var indexedDocumentAfterUpdateMu sync.Mutex
var indexedDocumentAfterUpdateHooks []IndexedDocumentHook

var indexedDocumentBeforeDeleteMu sync.Mutex
var indexedDocumentBeforeDeleteHooks []IndexedDocumentHook
var indexedDocumentAfterDeleteMu sync.Mutex
var indexedDocumentAfterDeleteHooks []IndexedDocumentHook

var indexedDocumentBeforeUpsertMu sync.Mutex
var indexedDocumentBeforeUpsertHooks []IndexedDocumentHook
var indexedDocumentAfterUpsertMu sync.Mutex
var indexedDocumentAfterUpsertHooks []IndexedDocumentHook

// doAfterSelectHooks executes all "after Select" hooks.
func (o *IndexedDocument) doAfterSelectHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentAfterSelectHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeInsertHooks executes all "before insert" hooks.
func (o *IndexedDocument) doBeforeInsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentBeforeInsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterInsertHooks executes all "after Insert" hooks.
func (o *IndexedDocument) doAfterInsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentAfterInsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeUpdateHooks executes all "before Update" hooks.
func (o *IndexedDocument) doBeforeUpdateHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentBeforeUpdateHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterUpdateHooks executes all "after Update" hooks.
func (o *IndexedDocument) doAfterUpdateHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentAfterUpdateHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeDeleteHooks executes all "before Delete" hooks.
func (o *IndexedDocument) doBeforeDeleteHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentBeforeDeleteHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterDeleteHooks executes all "after Delete" hooks.
func (o *IndexedDocument) doAfterDeleteHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentAfterDeleteHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doBeforeUpsertHooks executes all "before Upsert" hooks.
func (o *IndexedDocument) doBeforeUpsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentBeforeUpsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// doAfterUpsertHooks executes all "after Upsert" hooks.
func (o *IndexedDocument) doAfterUpsertHooks(ctx context.Context, exec boil.ContextExecutor) (err error) {
	if boil.HooksAreSkipped(ctx) {
		return nil
	}

	for _, hook := range indexedDocumentAfterUpsertHooks {
		if err := hook(ctx, exec, o); err != nil {
			return err
		}
	}

	return nil
}

// AddIndexedDocumentHook registers your hook function for all future operations.
func AddIndexedDocumentHook(hookPoint boil.HookPoint, indexedDocumentHook IndexedDocumentHook) {
	switch hookPoint {
	case boil.AfterSelectHook:
		indexedDocumentAfterSelectMu.Lock()
		indexedDocumentAfterSelectHooks = append(indexedDocumentAfterSelectHooks, indexedDocumentHook)
		indexedDocumentAfterSelectMu.Unlock()
	case boil.BeforeInsertHook:
		indexedDocumentBeforeInsertMu.Lock()
		indexedDocumentBeforeInsertHooks = append(indexedDocumentBeforeInsertHooks, indexedDocumentHook)
		indexedDocumentBeforeInsertMu.Unlock()
	case boil.AfterInsertHook:
		indexedDocumentAfterInsertMu.Lock()
		indexedDocumentAfterInsertHooks = append(indexedDocumentAfterInsertHooks, indexedDocumentHook)
		indexedDocumentAfterInsertMu.Unlock()
	case boil.BeforeUpdateHook:
		indexedDocumentBeforeUpdateMu.Lock()
		indexedDocumentBeforeUpdateHooks = append(indexedDocumentBeforeUpdateHooks, indexedDocumentHook)
		indexedDocumentBeforeUpdateMu.Unlock()
	case boil.AfterUpdateHook:
		indexedDocumentAfterUpdateMu.Lock()
		indexedDocumentAfterUpdateHooks = append(indexedDocumentAfterUpdateHooks, indexedDocumentHook)
		indexedDocumentAfterUpdateMu.Unlock()
	case boil.BeforeDeleteHook:
		indexedDocumentBeforeDeleteMu.Lock()
		indexedDocumentBeforeDeleteHooks = append(indexedDocumentBeforeDeleteHooks, indexedDocumentHook)
		indexedDocumentBeforeDeleteMu.Unlock()
	case boil.AfterDeleteHook:
		indexedDocumentAfterDeleteMu.Lock()
		indexedDocumentAfterDeleteHooks = append(indexedDocumentAfterDeleteHooks, indexedDocumentHook)
		indexedDocumentAfterDeleteMu.Unlock()
	case boil.BeforeUpsertHook:
		indexedDocumentBeforeUpsertMu.Lock()
		indexedDocumentBeforeUpsertHooks = append(indexedDocumentBeforeUpsertHooks, indexedDocumentHook)
		indexedDocumentBeforeUpsertMu.Unlock()
	case boil.AfterUpsertHook:
		indexedDocumentAfterUpsertMu.Lock()
		indexedDocumentAfterUpsertHooks = append(indexedDocumentAfterUpsertHooks, indexedDocumentHook)
		indexedDocumentAfterUpsertMu.Unlock()
	}
}

// One returns a single indexedDocument record from the query.
func (q indexedDocumentQuery) One(ctx context.Context, exec boil.ContextExecutor) (*IndexedDocument, error) {
	o := &IndexedDocument{}

	queries.SetLimit(q.Query, 1)

	err := q.Bind(ctx, exec, o)
	if err != nil {
		if errors.Is(err, sql.ErrNoRows) {
			return nil, sql.ErrNoRows
		}
		return nil, errors.Wrap(err, "sqlboiler: failed to execute a one query for indexed_documents")
	}

	if err := o.doAfterSelectHooks(ctx, exec); err != nil {
		return o, err
	}

	return o, nil
}

// All returns all IndexedDocument records from the query.
func (q indexedDocumentQuery) All(ctx context.Context, exec boil.ContextExecutor) (IndexedDocumentSlice, error) {
	var o []*IndexedDocument

	err := q.Bind(ctx, exec, &o)
	if err != nil {
		return nil, errors.Wrap(err, "sqlboiler: failed to assign all query results to IndexedDocument slice")
	}

	if len(indexedDocumentAfterSelectHooks) != 0 {
		for _, obj := range o {
			if err := obj.doAfterSelectHooks(ctx, exec); err != nil {
				return o, err
			}
		}
	}

	return o, nil
}

// Count returns the count of all IndexedDocument records in the query.
func (q indexedDocumentQuery) Count(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	var count int64

	queries.SetSelect(q.Query, nil)
	queries.SetCount(q.Query)

	err := q.Query.QueryRowContext(ctx, exec).Scan(&count)
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: failed to count indexed_documents rows")
	}

	return count, nil
}

// Exists checks if the row exists in the table.
func (q indexedDocumentQuery) Exists(ctx context.Context, exec boil.ContextExecutor) (bool, error) {
	var count int64

	queries.SetSelect(q.Query, nil)
	queries.SetCount(q.Query)
	queries.SetLimit(q.Query, 1)

	err := q.Query.QueryRowContext(ctx, exec).Scan(&count)
	if err != nil {
		return false, errors.Wrap(err, "sqlboiler: failed to check if indexed_documents exists")
	}

	return count > 0, nil
}

// IndexedDocuments retrieves all the records using an executor.
func IndexedDocuments(mods ...qm.QueryMod) indexedDocumentQuery {
	mods = append(mods, qm.From("\"schema_knowledge\".\"indexed_documents\""))
	q := NewQuery(mods...)
	if len(queries.GetSelect(q)) == 0 {
		queries.SetSelect(q, []string{"\"schema_knowledge\".\"indexed_documents\".*"})
	}

	return indexedDocumentQuery{q}
}

// FindIndexedDocument retrieves a single record by ID with an executor.
// If selectCols is empty Find will return all columns.
func FindIndexedDocument(ctx context.Context, exec boil.ContextExecutor, iD string, selectCols ...string) (*IndexedDocument, error) {
	indexedDocumentObj := &IndexedDocument{}

	sel := "*"
	if len(selectCols) > 0 {
		sel = strings.Join(strmangle.IdentQuoteSlice(dialect.LQ, dialect.RQ, selectCols), ",")
	}
	query := fmt.Sprintf(
		"select %s from \"schema_knowledge\".\"indexed_documents\" where \"id\"=$1", sel,
	)

	q := queries.Raw(query, iD)

	err := q.Bind(ctx, exec, indexedDocumentObj)
	if err != nil {
		if errors.Is(err, sql.ErrNoRows) {
			return nil, sql.ErrNoRows
		}
		return nil, errors.Wrap(err, "sqlboiler: unable to select from indexed_documents")
	}

	if err = indexedDocumentObj.doAfterSelectHooks(ctx, exec); err != nil {
		return indexedDocumentObj, err
	}

	return indexedDocumentObj, nil
}

// Insert a single record using an executor.
// See boil.Columns.InsertColumnSet documentation to understand column list inference for inserts.
func (o *IndexedDocument) Insert(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) error {
	if o == nil {
		return errors.New("sqlboiler: no indexed_documents provided for insertion")
	}

	var err error
	if !boil.TimestampsAreSkipped(ctx) {
		currTime := time.Now().In(boil.GetLocation())

		if queries.MustTime(o.CreatedAt).IsZero() {
			queries.SetScanner(&o.CreatedAt, currTime)
		}
		if queries.MustTime(o.UpdatedAt).IsZero() {
			queries.SetScanner(&o.UpdatedAt, currTime)
		}
	}

	if err := o.doBeforeInsertHooks(ctx, exec); err != nil {
		return err
	}

	nzDefaults := queries.NonZeroDefaultSet(indexedDocumentColumnsWithDefault, o)

	key := makeCacheKey(columns, nzDefaults)
	indexedDocumentInsertCacheMut.RLock()
	cache, cached := indexedDocumentInsertCache[key]
	indexedDocumentInsertCacheMut.RUnlock()

	if !cached {
		wl, returnColumns := columns.InsertColumnSet(
			indexedDocumentAllColumns,
			indexedDocumentColumnsWithDefault,
			indexedDocumentColumnsWithoutDefault,
			nzDefaults,
		)

		cache.valueMapping, err = queries.BindMapping(indexedDocumentType, indexedDocumentMapping, wl)
		if err != nil {
			return err
		}
		cache.retMapping, err = queries.BindMapping(indexedDocumentType, indexedDocumentMapping, returnColumns)
		if err != nil {
			return err
		}
		if len(wl) != 0 {
			cache.query = fmt.Sprintf("INSERT INTO \"schema_knowledge\".\"indexed_documents\" (\"%s\") %%sVALUES (%s)%%s", strings.Join(wl, "\",\""), strmangle.Placeholders(dialect.UseIndexPlaceholders, len(wl), 1, 1))
		} else {
			cache.query = "INSERT INTO \"schema_knowledge\".\"indexed_documents\" %sDEFAULT VALUES%s"
		}

		var queryOutput, queryReturning string

		if len(cache.retMapping) != 0 {
			queryReturning = fmt.Sprintf(" RETURNING \"%s\"", strings.Join(returnColumns, "\",\""))
		}

		cache.query = fmt.Sprintf(cache.query, queryOutput, queryReturning)
	}

	value := reflect.Indirect(reflect.ValueOf(o))
	vals := queries.ValuesFromMapping(value, cache.valueMapping)

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, vals)
	}

	if len(cache.retMapping) != 0 {
		err = exec.QueryRowContext(ctx, cache.query, vals...).Scan(queries.PtrsFromMapping(value, cache.retMapping)...)
	} else {
		_, err = exec.ExecContext(ctx, cache.query, vals...)
	}

	if err != nil {
		return errors.Wrap(err, "sqlboiler: unable to insert into indexed_documents")
	}

	if !cached {
		indexedDocumentInsertCacheMut.Lock()
		indexedDocumentInsertCache[key] = cache
		indexedDocumentInsertCacheMut.Unlock()
	}

	return o.doAfterInsertHooks(ctx, exec)
}

// Update uses an executor to update the IndexedDocument.
// See boil.Columns.UpdateColumnSet documentation to understand column list inference for updates.
// Update does not automatically update the record in case of default values. Use .Reload() to refresh the records.
func (o *IndexedDocument) Update(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) (int64, error) {
	if !boil.TimestampsAreSkipped(ctx) {
		currTime := time.Now().In(boil.GetLocation())

		queries.SetScanner(&o.UpdatedAt, currTime)
	}

	var err error
	if err = o.doBeforeUpdateHooks(ctx, exec); err != nil {
		return 0, err
	}
	key := makeCacheKey(columns, nil)
	indexedDocumentUpdateCacheMut.RLock()
	cache, cached := indexedDocumentUpdateCache[key]
	indexedDocumentUpdateCacheMut.RUnlock()

	if !cached {
		wl := columns.UpdateColumnSet(
			indexedDocumentAllColumns,
			indexedDocumentPrimaryKeyColumns,
		)

		if !columns.IsWhitelist() {
			wl = strmangle.SetComplement(wl, []string{"created_at"})
		}
		if len(wl) == 0 {
			return 0, errors.New("sqlboiler: unable to update indexed_documents, could not build whitelist")
		}

		cache.query = fmt.Sprintf("UPDATE \"schema_knowledge\".\"indexed_documents\" SET %s WHERE %s",
			strmangle.SetParamNames("\"", "\"", 1, wl),
			strmangle.WhereClause("\"", "\"", len(wl)+1, indexedDocumentPrimaryKeyColumns),
		)
		cache.valueMapping, err = queries.BindMapping(indexedDocumentType, indexedDocumentMapping, append(wl, indexedDocumentPrimaryKeyColumns...))
		if err != nil {
			return 0, err
		}
	}

	values := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(o)), cache.valueMapping)

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, values)
	}
	var result sql.Result
	result, err = exec.ExecContext(ctx, cache.query, values...)
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: unable to update indexed_documents row")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: failed to get rows affected by update for indexed_documents")
	}

	if !cached {
		indexedDocumentUpdateCacheMut.Lock()
		indexedDocumentUpdateCache[key] = cache
		indexedDocumentUpdateCacheMut.Unlock()
	}

	return rowsAff, o.doAfterUpdateHooks(ctx, exec)
}

// UpdateAll updates all rows with the specified column values.
func (q indexedDocumentQuery) UpdateAll(ctx context.Context, exec boil.ContextExecutor, cols M) (int64, error) {
	queries.SetUpdate(q.Query, cols)

	result, err := q.Query.ExecContext(ctx, exec)
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: unable to update all for indexed_documents")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: unable to retrieve rows affected for indexed_documents")
	}

	return rowsAff, nil
}

// UpdateAll updates all rows with the specified column values, using an executor.
func (o IndexedDocumentSlice) UpdateAll(ctx context.Context, exec boil.ContextExecutor, cols M) (int64, error) {
	ln := int64(len(o))
	if ln == 0 {
		return 0, nil
	}

	if len(cols) == 0 {
		return 0, errors.New("sqlboiler: update all requires at least one column argument")
	}

	colNames := make([]string, len(cols))
	args := make([]any, len(cols))

	i := 0
	for name, value := range cols {
		colNames[i] = name
		args[i] = value
		i++
	}

	// Append all of the primary key values for each column
	for _, obj := range o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), indexedDocumentPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := fmt.Sprintf("UPDATE \"schema_knowledge\".\"indexed_documents\" SET %s WHERE %s",
		strmangle.SetParamNames("\"", "\"", 1, colNames),
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), len(colNames)+1, indexedDocumentPrimaryKeyColumns, len(o)))

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args...)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: unable to update all in indexedDocument slice")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: unable to retrieve rows affected all in update all indexedDocument")
	}
	return rowsAff, nil
}

// Upsert attempts an insert using an executor, and does an update or ignore on conflict.
// See boil.Columns documentation for how to properly use updateColumns and insertColumns.
func (o *IndexedDocument) Upsert(ctx context.Context, exec boil.ContextExecutor, updateOnConflict bool, conflictColumns []string, updateColumns, insertColumns boil.Columns, opts ...UpsertOptionFunc) error {
	if o == nil {
		return errors.New("sqlboiler: no indexed_documents provided for upsert")
	}
	if !boil.TimestampsAreSkipped(ctx) {
		currTime := time.Now().In(boil.GetLocation())

		if queries.MustTime(o.CreatedAt).IsZero() {
			queries.SetScanner(&o.CreatedAt, currTime)
		}
		queries.SetScanner(&o.UpdatedAt, currTime)
	}

	if err := o.doBeforeUpsertHooks(ctx, exec); err != nil {
		return err
	}

	nzDefaults := queries.NonZeroDefaultSet(indexedDocumentColumnsWithDefault, o)

	// Build cache key in-line uglily - mysql vs psql problems
	buf := strmangle.GetBuffer()
	if updateOnConflict {
		buf.WriteByte('t')
	} else {
		buf.WriteByte('f')
	}
	buf.WriteByte('.')
	for _, c := range conflictColumns {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	buf.WriteString(strconv.Itoa(updateColumns.Kind))
	for _, c := range updateColumns.Cols {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	buf.WriteString(strconv.Itoa(insertColumns.Kind))
	for _, c := range insertColumns.Cols {
		buf.WriteString(c)
	}
	buf.WriteByte('.')
	for _, c := range nzDefaults {
		buf.WriteString(c)
	}
	key := buf.String()
	strmangle.PutBuffer(buf)

	indexedDocumentUpsertCacheMut.RLock()
	cache, cached := indexedDocumentUpsertCache[key]
	indexedDocumentUpsertCacheMut.RUnlock()

	var err error

	if !cached {
		insert, _ := insertColumns.InsertColumnSet(
			indexedDocumentAllColumns,
			indexedDocumentColumnsWithDefault,
			indexedDocumentColumnsWithoutDefault,
			nzDefaults,
		)

		update := updateColumns.UpdateColumnSet(
			indexedDocumentAllColumns,
			indexedDocumentPrimaryKeyColumns,
		)

		if updateOnConflict && len(update) == 0 {
			return errors.New("sqlboiler: unable to upsert indexed_documents, could not build update column list")
		}

		ret := strmangle.SetComplement(indexedDocumentAllColumns, strmangle.SetIntersect(insert, update))

		conflict := conflictColumns
		if len(conflict) == 0 && updateOnConflict && len(update) != 0 {
			if len(indexedDocumentPrimaryKeyColumns) == 0 {
				return errors.New("sqlboiler: unable to upsert indexed_documents, could not build conflict column list")
			}

			conflict = make([]string, len(indexedDocumentPrimaryKeyColumns))
			copy(conflict, indexedDocumentPrimaryKeyColumns)
		}
		cache.query = buildUpsertQueryPostgres(dialect, "\"schema_knowledge\".\"indexed_documents\"", updateOnConflict, ret, update, conflict, insert, opts...)

		cache.valueMapping, err = queries.BindMapping(indexedDocumentType, indexedDocumentMapping, insert)
		if err != nil {
			return err
		}
		if len(ret) != 0 {
			cache.retMapping, err = queries.BindMapping(indexedDocumentType, indexedDocumentMapping, ret)
			if err != nil {
				return err
			}
		}
	}

	value := reflect.Indirect(reflect.ValueOf(o))
	vals := queries.ValuesFromMapping(value, cache.valueMapping)
	var returns []any
	if len(cache.retMapping) != 0 {
		returns = queries.PtrsFromMapping(value, cache.retMapping)
	}

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, cache.query)
		fmt.Fprintln(writer, vals)
	}
	if len(cache.retMapping) != 0 {
		err = exec.QueryRowContext(ctx, cache.query, vals...).Scan(returns...)
		if errors.Is(err, sql.ErrNoRows) {
			err = nil // Postgres doesn't return anything when there's no update
		}
	} else {
		_, err = exec.ExecContext(ctx, cache.query, vals...)
	}
	if err != nil {
		return errors.Wrap(err, "sqlboiler: unable to upsert indexed_documents")
	}

	if !cached {
		indexedDocumentUpsertCacheMut.Lock()
		indexedDocumentUpsertCache[key] = cache
		indexedDocumentUpsertCacheMut.Unlock()
	}

	return o.doAfterUpsertHooks(ctx, exec)
}

// Delete deletes a single IndexedDocument record with an executor.
// Delete will match against the primary key column to find the record to delete.
func (o *IndexedDocument) Delete(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if o == nil {
		return 0, errors.New("sqlboiler: no IndexedDocument provided for delete")
	}

	if err := o.doBeforeDeleteHooks(ctx, exec); err != nil {
		return 0, err
	}

	args := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(o)), indexedDocumentPrimaryKeyMapping)
	sql := "DELETE FROM \"schema_knowledge\".\"indexed_documents\" WHERE \"id\"=$1"

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args...)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: unable to delete from indexed_documents")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: failed to get rows affected by delete for indexed_documents")
	}

	if err := o.doAfterDeleteHooks(ctx, exec); err != nil {
		return 0, err
	}

	return rowsAff, nil
}

// DeleteAll deletes all matching rows.
func (q indexedDocumentQuery) DeleteAll(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if q.Query == nil {
		return 0, errors.New("sqlboiler: no indexedDocumentQuery provided for delete all")
	}

	queries.SetDelete(q.Query)

	result, err := q.Query.ExecContext(ctx, exec)
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: unable to delete all from indexed_documents")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: failed to get rows affected by deleteall for indexed_documents")
	}

	return rowsAff, nil
}

// DeleteAll deletes all rows in the slice, using an executor.
func (o IndexedDocumentSlice) DeleteAll(ctx context.Context, exec boil.ContextExecutor) (int64, error) {
	if len(o) == 0 {
		return 0, nil
	}

	if len(indexedDocumentBeforeDeleteHooks) != 0 {
		for _, obj := range o {
			if err := obj.doBeforeDeleteHooks(ctx, exec); err != nil {
				return 0, err
			}
		}
	}

	var args []any
	for _, obj := range o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), indexedDocumentPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := "DELETE FROM \"schema_knowledge\".\"indexed_documents\" WHERE " +
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), 1, indexedDocumentPrimaryKeyColumns, len(o))

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, args)
	}
	result, err := exec.ExecContext(ctx, sql, args...)
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: unable to delete all from indexedDocument slice")
	}

	rowsAff, err := result.RowsAffected()
	if err != nil {
		return 0, errors.Wrap(err, "sqlboiler: failed to get rows affected by deleteall for indexed_documents")
	}

	if len(indexedDocumentAfterDeleteHooks) != 0 {
		for _, obj := range o {
			if err := obj.doAfterDeleteHooks(ctx, exec); err != nil {
				return 0, err
			}
		}
	}

	return rowsAff, nil
}

// Reload refetches the object from the database
// using the primary keys with an executor.
func (o *IndexedDocument) Reload(ctx context.Context, exec boil.ContextExecutor) error {
	ret, err := FindIndexedDocument(ctx, exec, o.ID)
	if err != nil {
		return err
	}

	*o = *ret
	return nil
}

// ReloadAll refetches every row with matching primary key column values
// and overwrites the original object slice with the newly updated slice.
func (o *IndexedDocumentSlice) ReloadAll(ctx context.Context, exec boil.ContextExecutor) error {
	if o == nil || len(*o) == 0 {
		return nil
	}

	slice := IndexedDocumentSlice{}
	var args []any
	for _, obj := range *o {
		pkeyArgs := queries.ValuesFromMapping(reflect.Indirect(reflect.ValueOf(obj)), indexedDocumentPrimaryKeyMapping)
		args = append(args, pkeyArgs...)
	}

	sql := "SELECT \"schema_knowledge\".\"indexed_documents\".* FROM \"schema_knowledge\".\"indexed_documents\" WHERE " +
		strmangle.WhereClauseRepeated(string(dialect.LQ), string(dialect.RQ), 1, indexedDocumentPrimaryKeyColumns, len(*o))

	q := queries.Raw(sql, args...)

	err := q.Bind(ctx, exec, &slice)
	if err != nil {
		return errors.Wrap(err, "sqlboiler: unable to reload all in IndexedDocumentSlice")
	}

	*o = slice

	return nil
}

// IndexedDocumentExists checks if the IndexedDocument row exists.
func IndexedDocumentExists(ctx context.Context, exec boil.ContextExecutor, iD string) (bool, error) {
	var exists bool
	sql := "select exists(select 1 from \"schema_knowledge\".\"indexed_documents\" where \"id\"=$1 limit 1)"

	if boil.IsDebug(ctx) {
		writer := boil.DebugWriterFrom(ctx)
		fmt.Fprintln(writer, sql)
		fmt.Fprintln(writer, iD)
	}
	row := exec.QueryRowContext(ctx, sql, iD)

	err := row.Scan(&exists)
	if err != nil {
		return false, errors.Wrap(err, "sqlboiler: unable to check if indexed_documents exists")
	}

	return exists, nil
}

// Exists checks if the IndexedDocument row exists.
func (o *IndexedDocument) Exists(ctx context.Context, exec boil.ContextExecutor) (bool, error) {
	return IndexedDocumentExists(ctx, exec, o.ID)
}
